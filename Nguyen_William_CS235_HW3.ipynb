{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5Th-5nwu91sy"
      },
      "outputs": [],
      "source": [
        "# HW 3 - NYT Articles Notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OCEdoudKNaM",
        "outputId": "6b27adc2-68b8-45a5-eb49-659942b11860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text   label\n",
            "0  (reuters) - carlos tevez sealed his move to ju...  sports\n",
            "1  if professional pride and strong defiance can ...  sports\n",
            "2  palermo, sicily â€” roberta vinci beat top-seede...  sports\n",
            "3  spain's big two soccer teams face a pair of it...  sports\n",
            "4  the argentine soccer club san lorenzo complete...  sports\n",
            "(11519, 2)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"nyt.csv\")\n",
        "print(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_KXrg5F325M"
      },
      "source": [
        "Part 1: Bag Of Words (20 points):\n",
        "\n",
        "(a) binary-valued vector\n",
        "\n",
        "(b) frequency vector\n",
        "\n",
        "(c) tf-idf vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Logistic Regression and split data 80-10-10\n",
        "\n",
        "# Define X and y \n",
        "X = df[\"text\"].tolist()\n",
        "y = df[\"label\"].tolist()\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# 90 - 10 split on entire dataset to get testing dataset (10%) and train-val dataset (90%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Split the train-val dataset (90%) into an 80 - 10 split, (80% train, 10% val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(10/90), random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "xfFDGu5H4IWs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9533235897080291\n"
          ]
        }
      ],
      "source": [
        "# (a)binary-valued vector\n",
        "# TODO\n",
        "\n",
        "# Initialize CountVectorizer and specify Binary = True\n",
        "binary_vec = CountVectorizer(binary=True)\n",
        "\n",
        "# Fit transform the tranining data and transform val and test data\n",
        "X_train_bvec = binary_vec.fit_transform(X_train)\n",
        "\n",
        "X_val_bvec = binary_vec.transform(X_val)\n",
        "X_test_bvec = binary_vec.transform(X_test)\n",
        "\n",
        "# Fit Linear Regression model and generate predictions based on X_val\n",
        "lr.fit(X_train_bvec, y_train)\n",
        "preds_bvec = lr.predict(X_val_bvec)\n",
        "\n",
        "# Find macro f1 score (since this is binary classification)\n",
        "print(f1_score(y_val, preds_bvec, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "3zw5t27sKoWx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9618116952759584\n"
          ]
        }
      ],
      "source": [
        "# (b) frequency vector\n",
        "# TODO\n",
        "\n",
        "# Initialize CountVectorizer and set Binary = False for frequency/ count vectors\n",
        "count_vec = CountVectorizer(binary=False)\n",
        "\n",
        "# Fit transform the tranining data and transform val and test data\n",
        "# change bvec (binary vector) to fvec (frequency vector)\n",
        "X_train_fvec = count_vec.fit_transform(X_train)\n",
        "\n",
        "X_val_fvec = count_vec.transform(X_val)\n",
        "X_test_fvec = count_vec.transform(X_test)\n",
        "\n",
        "# Fit Linear Regression model and generate predictions based on X_val\n",
        "lr.fit(X_train_fvec, y_train)\n",
        "preds_fvec = lr.predict(X_val_fvec)\n",
        "\n",
        "# Find macro f1 score (since this is binary classification)\n",
        "print(f1_score(y_val, preds_fvec, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "VX3vTT5V4Lz2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9633348981778518\n"
          ]
        }
      ],
      "source": [
        "# (c) tf-idf vector\n",
        "# TODO\n",
        "\n",
        "# Initialize TfidfVectorizer, lowercase all words, and set stop words to english\n",
        "tfidf_vec = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
        "\n",
        "# Fit transform the tranining data and transform val and test data\n",
        "# change bvec (binary vector)/ fvec (frequency vector) to tfidf\n",
        "X_train_tfidf = tfidf_vec.fit_transform(X_train)\n",
        "\n",
        "X_val_tfidf = tfidf_vec.transform(X_val)\n",
        "X_test_tfidf = tfidf_vec.transform(X_test)\n",
        "\n",
        "# Fit Linear Regression model and generate predictions based on X_val\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "preds_tfidf = lr.predict(X_val_tfidf)\n",
        "\n",
        "# Find macro f1 score (since this is binary classification)\n",
        "print(f1_score(y_val, preds_tfidf, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja_LaTWB4NzG"
      },
      "source": [
        "Part 2: Word2Vec (20 points):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "bsN143oO4Rgy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9458282679118274\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "\n",
        "# Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize sentences (lowercase and split)\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Apply tokenization to X train, val and test\n",
        "X_train_token = [tokenize(sentence) for sentence in X_train]\n",
        "X_val_token = [tokenize(sentence) for sentence in X_val]\n",
        "X_test_token = [tokenize(sentence) for sentence in X_test]\n",
        "\n",
        "# Initialize word2vec model and set dimensions = 100\n",
        "wv = Word2Vec(\n",
        "    sentences=X_train_token,\n",
        "    vector_size=100\n",
        ")\n",
        "\n",
        "# Iterate through every sentance in the dataset and convert into a sincle vector\n",
        "# Essentially averages all word vectors into a single 100 dimension document embedding\n",
        "def tokens_to_vector(token, model, size=100):\n",
        "    words = [word for word in token if word in model.wv]\n",
        "    if not words:\n",
        "        return np.zeros(size)\n",
        "    return np.mean(model.wv[words], axis=0)\n",
        "\n",
        "# Apply token to vector onto X train, val and test\n",
        "X_train_vec = np.vstack([tokens_to_vector(tokens, wv) for tokens in X_train_token])\n",
        "X_val_vec   = np.vstack([tokens_to_vector(tokens, wv) for tokens in X_val_token])\n",
        "X_test_vec  = np.vstack([tokens_to_vector(tokens, wv) for tokens in X_test_token])\n",
        "\n",
        "# Train logistic regression model and calculate f1 score\n",
        "lr.fit(X_train_vec, y_train)\n",
        "word2vec_pred = lr.predict(X_val_vec)\n",
        "print(f1_score(y_val, word2vec_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9416854878911732\n"
          ]
        }
      ],
      "source": [
        "# GloVe\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Initialize GloVe model\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\") # glove-wiki is a pretrained model, trained on wikipidia and gigword\n",
        "\n",
        "# Convert tokens into vectors\n",
        "def to_glove_vector(tokens, model, size=100):\n",
        "    vectors = [model[word] for word in tokens if word in model]\n",
        "    if not vectors:\n",
        "        return np.zeros(size) # defualt to 0 for unknown words\n",
        "    return np.mean(vectors, axis=0)\n",
        "# Loops through tokenized sentences and only keeps words that exist in the GloVe model's vocab, else set as 0\n",
        "\n",
        "# Apply conversion to X train, test, and val\n",
        "X_train_vec = np.vstack([to_glove_vector(tokens, glove_model) for tokens in X_train_token])\n",
        "X_val_vec   = np.vstack([to_glove_vector(tokens, glove_model) for tokens in X_val_token])\n",
        "X_test_vec  = np.vstack([to_glove_vector(tokens, glove_model) for tokens in X_test_token])\n",
        "\n",
        "#  Train logistic regression model and calculate f1 score\n",
        "lr.fit(X_train_vec, y_train)\n",
        "glove_pred = lr.predict(X_val_vec)\n",
        "print(f1_score(y_val, glove_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5o0icLb4R8-"
      },
      "source": [
        "Part 3: BERT (20 points):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform train-test-split again but this time encode label values using label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"label\"])\n",
        "\n",
        "# Define X and y \n",
        "X = df[\"text\"].tolist()\n",
        "y = df[\"label\"].tolist()\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# 90 - 10 split on entire dataset to get testing dataset (10%) and train-val dataset (90%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Split the train-val dataset (90%) into an 80 - 10 split, (80% train, 10% val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(10/90), random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "import torch\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.models.bert import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Initialize Bert Tokenizer - load pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define custom dataset\n",
        "class TextData(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    # Returns the number of samples\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    # Converts items into BERT compatable format\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Apply tokenizer to convert text into input ids for BERT\n",
        "        tokenized_data = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=64, # set maximum length to 64\n",
        "            return_tensors=\"pt\" # returns pytorch tensors\n",
        "        )\n",
        "\n",
        "        item = {key: val.squeeze(0) for key,val in tokenized_data.items()}\n",
        "        item[\"labels\"] = torch.tensor(label, dtype=torch.long) # Add label to the returned dictionary\n",
        "        return item\n",
        "    \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=3 # set for 3 output classes (sports, politics, business)\n",
        ")\n",
        "\n",
        "# Uses GPU if available, if not default to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Apply custom dataset configurations to train, val, and test data\n",
        "train_data = TextData(X_train, y_train, tokenizer)\n",
        "val_data = TextData(X_val, y_val, tokenizer)\n",
        "test_data = TextData(X_test, y_test, tokenizer)\n",
        "\n",
        "# Create batches of tokenized data\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=16)\n",
        "test_loader = DataLoader(test_data, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "# Define 3 epoch\n",
        "# Use adam and optimization algorithm\n",
        "epochs = 3\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Calculate total number of gradient updates\n",
        "total_steps = len(train_loader)*epochs\n",
        "\n",
        "# Decrease lr gradually\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/ 3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23939f56e61b4f1f84c2cd94a43a450d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/576 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 loss: 0.22\n",
            "Epoch 2/ 3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccabb5d67b0c48f2b7553360567ba531",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/576 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 loss: 0.00\n",
            "Epoch 3/ 3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b28760a0b7734d3db3b0f8dbe5174225",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/576 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 loss: 0.00\n"
          ]
        }
      ],
      "source": [
        "model.train() # Set to training mode\n",
        "\n",
        "# Define training loop - loops through per epoch per batch\n",
        "# Calculates and minimizes gradients\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/ {epochs}\")\n",
        "    \n",
        "    # Use tqdm for progress bar\n",
        "    for batch in tqdm(train_loader):\n",
        "        \n",
        "        batch = {k: v.to(device) for k, v in batch.items()} # move inputs to GPU\n",
        "\n",
        "        # Computs outputs and loss\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward() # backpropagation\n",
        "        optimizer.step() # update model weights\n",
        "        scheduler.step() # adjust lr\n",
        "        optimizer.zero_grad() # reset gradient and repeat until loop finishes\n",
        "\n",
        "    print(f\"Epoch {epoch+1} loss: {loss.item():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Create a list to store predictions and labels\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "# Loops through model and extracts predictions and outputs. Appends to predictions and labels list\n",
        "with torch.no_grad(): # Diable gradient updates/ calculations for evaluation\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k,v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits # extract raw outputs\n",
        "        preds = torch.argmax(logits, dim=1) # extract predicted class\n",
        "\n",
        "        # Moves to CPU and stores data in list\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        labels.extend(batch[\"labels\"].cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9722\n",
            "0.9458\n",
            "0.9722\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Calculate evaluation metrics (accuracy, f1_macro, and f1_micro)\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "macro_f1 = f1_score(labels, predictions, average=\"macro\")\n",
        "micro_f1 = f1_score(labels, predictions, average=\"micro\")\n",
        "\n",
        "print(round(accuracy, 4))\n",
        "print(round(macro_f1, 4))\n",
        "print(round(micro_f1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYqEGP9D4ZX7"
      },
      "source": [
        "Part 4: Summary of results / Reflection\n",
        "## TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "2    8639\n",
              "1    1451\n",
              "0    1429\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Bert]\n",
        "- Based on Accuracy and F1 Micro Score, 97.05% of all predictions were correctly classified. Similarly,\n",
        "- Based on Macro F1 Score of 94.36%, it suggests all classess performed relatively well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
