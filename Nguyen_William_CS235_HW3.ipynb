{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Th-5nwu91sy"
      },
      "outputs": [],
      "source": [
        "# HW 3 - NYT Articles Notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OCEdoudKNaM",
        "outputId": "6b27adc2-68b8-45a5-eb49-659942b11860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text   label\n",
            "0  (reuters) - carlos tevez sealed his move to ju...  sports\n",
            "1  if professional pride and strong defiance can ...  sports\n",
            "2  palermo, sicily â€” roberta vinci beat top-seede...  sports\n",
            "3  spain's big two soccer teams face a pair of it...  sports\n",
            "4  the argentine soccer club san lorenzo complete...  sports\n",
            "(11519, 2)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"nyt.csv\")\n",
        "print(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_KXrg5F325M"
      },
      "source": [
        "Part 1: Bag Of Words (20 points):\n",
        "\n",
        "(a) binary-valued vector\n",
        "\n",
        "(b) frequency vector\n",
        "\n",
        "(c) tf-idf vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Logistic Regression and split data 80-10-10\n",
        "\n",
        "# Define X and y \n",
        "X = df[\"text\"].tolist()\n",
        "y = df[\"label\"].tolist()\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# 90 - 10 split on entire dataset to get testing dataset (10%) and train-val dataset (90%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Split the train-val dataset (90%) into an 80 - 10 split, (80% train, 10% val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(10/90), random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xfFDGu5H4IWs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9533235897080291\n"
          ]
        }
      ],
      "source": [
        "# (a)binary-valued vector\n",
        "# TODO\n",
        "\n",
        "# Initialize CountVectorizer and specify Binary = True\n",
        "binary_vec = CountVectorizer(binary=True)\n",
        "\n",
        "# Fit transform the tranining data and transform val and test data\n",
        "X_train_bvec = binary_vec.fit_transform(X_train)\n",
        "\n",
        "X_val_bvec = binary_vec.transform(X_val)\n",
        "X_test_bvec = binary_vec.transform(X_test)\n",
        "\n",
        "# Fit Linear Regression model and generate predictions based on X_val\n",
        "lr.fit(X_train_bvec, y_train)\n",
        "preds_bvec = lr.predict(X_val_bvec)\n",
        "\n",
        "# Find macro f1 score (since this is binary classification)\n",
        "print(f1_score(y_val, preds_bvec, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3zw5t27sKoWx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9618116952759584\n"
          ]
        }
      ],
      "source": [
        "# (b) frequency vector\n",
        "# TODO\n",
        "\n",
        "# Initialize CountVectorizer and set Binary = False for frequency/ count vectors\n",
        "count_vec = CountVectorizer(binary=False)\n",
        "\n",
        "# Fit transform the tranining data and transform val and test data\n",
        "# change bvec (binary vector) to fvec (frequency vector)\n",
        "X_train_fvec = count_vec.fit_transform(X_train)\n",
        "\n",
        "X_val_fvec = count_vec.transform(X_val)\n",
        "X_test_fvec = count_vec.transform(X_test)\n",
        "\n",
        "# Fit Linear Regression model and generate predictions based on X_val\n",
        "lr.fit(X_train_fvec, y_train)\n",
        "preds_fvec = lr.predict(X_val_fvec)\n",
        "\n",
        "# Find macro f1 score (since this is binary classification)\n",
        "print(f1_score(y_val, preds_fvec, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VX3vTT5V4Lz2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9633348981778518\n"
          ]
        }
      ],
      "source": [
        "# (c) tf-idf vector\n",
        "# TODO\n",
        "\n",
        "# Initialize TfidfVectorizer, lowercase all words, and set stop words to english\n",
        "tfidf_vec = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
        "\n",
        "# Fit transform the tranining data and transform val and test data\n",
        "# change bvec (binary vector)/ fvec (frequency vector) to tfidf\n",
        "X_train_tfidf = tfidf_vec.fit_transform(X_train)\n",
        "\n",
        "X_val_tfidf = tfidf_vec.transform(X_val)\n",
        "X_test_tfidf = tfidf_vec.transform(X_test)\n",
        "\n",
        "# Fit Linear Regression model and generate predictions based on X_val\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "preds_tfidf = lr.predict(X_val_tfidf)\n",
        "\n",
        "# Find macro f1 score (since this is binary classification)\n",
        "print(f1_score(y_val, preds_tfidf, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja_LaTWB4NzG"
      },
      "source": [
        "Part 2: Word2Vec (20 points):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bsN143oO4Rgy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9458014408079007\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "\n",
        "# Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize sentences (lowercase and split)\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Apply tokenization to X train, val and test\n",
        "X_train_token = [tokenize(sentence) for sentence in X_train]\n",
        "X_val_token = [tokenize(sentence) for sentence in X_val]\n",
        "X_test_token = [tokenize(sentence) for sentence in X_test]\n",
        "\n",
        "# Initialize word2vec model and set dimensions = 100\n",
        "wv = Word2Vec(\n",
        "    sentences=X_train_token,\n",
        "    vector_size=100\n",
        ")\n",
        "\n",
        "# Iterate through every sentance in the dataset and convert into a sincle vector\n",
        "# Essentially averages all word vectors into a single 100 dimension document embedding\n",
        "def tokens_to_vector(token, model, size=100):\n",
        "    words = [word for word in token if word in model.wv]\n",
        "    if not words:\n",
        "        return np.zeros(size)\n",
        "    return np.mean(model.wv[words], axis=0)\n",
        "\n",
        "# Apply token to vector onto X train, val and test\n",
        "X_train_vec = np.vstack([tokens_to_vector(tokens, wv) for tokens in X_train_token])\n",
        "X_val_vec   = np.vstack([tokens_to_vector(tokens, wv) for tokens in X_val_token])\n",
        "X_test_vec  = np.vstack([tokens_to_vector(tokens, wv) for tokens in X_test_token])\n",
        "\n",
        "# Train logistic regression model and calculate f1 score\n",
        "lr.fit(X_train_vec, y_train)\n",
        "word2vec_pred = lr.predict(X_val_vec)\n",
        "print(f1_score(y_val, word2vec_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9416854878911732\n"
          ]
        }
      ],
      "source": [
        "# GloVe\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Initialize GloVe model\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\") # glove-wiki is a pretrained model, trained on wikipidia and gigword\n",
        "\n",
        "# Convert tokens into vectors\n",
        "def to_glove_vector(tokens, model, size=100):\n",
        "    vectors = [model[word] for word in tokens if word in model]\n",
        "    if not vectors:\n",
        "        return np.zeros(size) # defualt to 0 for unknown words\n",
        "    return np.mean(vectors, axis=0)\n",
        "# Loops through tokenized sentences and only keeps words that exist in the GloVe model's vocab, else set as 0\n",
        "\n",
        "# Apply conversion to X train, test, and val\n",
        "X_train_vec = np.vstack([to_glove_vector(tokens, glove_model) for tokens in X_train_token])\n",
        "X_val_vec   = np.vstack([to_glove_vector(tokens, glove_model) for tokens in X_val_token])\n",
        "X_test_vec  = np.vstack([to_glove_vector(tokens, glove_model) for tokens in X_test_token])\n",
        "\n",
        "#  Train logistic regression model and calculate f1 score\n",
        "lr.fit(X_train_vec, y_train)\n",
        "glove_pred = lr.predict(X_val_vec)\n",
        "print(f1_score(y_val, glove_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5o0icLb4R8-"
      },
      "source": [
        "Part 3: BERT (20 points):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform train-test-split again but this time encode label values using label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"label\"])\n",
        "\n",
        "# Define X and y \n",
        "X = df[\"text\"].tolist()\n",
        "y = df[\"label\"].tolist()\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# 90 - 10 split on entire dataset to get testing dataset (10%) and train-val dataset (90%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Split the train-val dataset (90%) into an 80 - 10 split, (80% train, 10% val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(10/90), random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e_-H1VIh4WCM"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.bert import BertTokenizer, BertModel\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "train_data = Dataset\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_data(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        trunication=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class TextData(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        tokenize = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"label\": label,\n",
        "            \"input_ids\": tokenize[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tokenize[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(label)\n",
        "        }\n",
        "    \n",
        "train_data = TextData(X_train, y_train)\n",
        "val_data = TextData(X_val, y_val)\n",
        "test_data = TextData(X_test, y_test)\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=3\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=16)\n",
        "test_loader = DataLoader(test_data, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYqEGP9D4ZX7"
      },
      "source": [
        "Part 4: Summary of results / Reflection\n",
        "## TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
